import{_ as l,C as u,c,o as a,a6 as n,j as t,b as p,a as o,t as s,w as r,G as g,a7 as m}from"./chunks/framework.fkSL6LTs.js";const R=JSON.parse('{"title":"How TeXRA Agents Work: An Overview","description":"","frontmatter":{},"headers":[],"relativePath":"guide/agent-architecture.md","filePath":"guide/agent-architecture.md"}'),f={name:"guide/agent-architecture.md"};function h(i,e,A,y,b,T){const d=u("Mermaid");return a(),c("div",null,[e[14]||(e[14]=n("",6)),t("ol",null,[e[8]||(e[8]=n("",1)),t("li",null,[e[6]||(e[6]=t("strong",null,[t("code",null,"prompts")],-1)),e[7]||(e[7]=o(": Contain text templates that TeXRA fills with your specific context (input files, instructions) to guide the LLM at different stages: ")),t("ul",null,[e[4]||(e[4]=t("li",null,[t("code",null,"systemPrompt"),o(": Sets the overall role and high-level instructions for the LLM.")],-1)),t("li",null,[e[0]||(e[0]=t("code",null,"userPrefix",-1)),e[1]||(e[1]=o(": Provides the main context, including your input file(s) (available via e.g., ")),t("code",null,s(i.INPUT_CONTENT),1),e[2]||(e[2]=o(") and the specific instruction you typed in the UI (available via ")),t("code",null,s(i.INSTRUCTION),1),e[3]||(e[3]=o(")."))]),e[5]||(e[5]=t("li",null,[t("code",null,"userRequest"),o(": Asks the LLM to perform the initial task (Round 0). Often instructs the LLM to think within "),t("code",null,"<scratchpad>"),o(" tags and then output the main content wrapped within the XML tags defined by "),t("code",null,"settings.documentTag"),o(" (e.g., "),t("code",null,"<document>...</document>"),o("). You can also provide an "),t("strong",null,"array"),o(" here: the first entry becomes the round 0 request, and any additional entries drive automatic reflection rounds (Round 1+). When a run consumes more rounds than entries you specify, the first reflection template is reused.")],-1))])])]),t("p",null,[e[9]||(e[9]=o("_(Prompts use Jinja2 templating. For a detailed list of available variables like ")),t("code",null,s(i.INPUT_CONTENT),1),e[10]||(e[10]=o(" and how to use them, see the ")),e[11]||(e[11]=t("a",{href:"./custom-agents.html"},"Custom Agents",-1)),e[12]||(e[12]=o(" guide.)*"))]),e[15]||(e[15]=n("",3)),(a(),p(m,null,{default:r(()=>[g(d,{id:"mermaid-78",class:"mermaid",graph:"sequenceDiagram%0A%20%20%20%20participant%20User%0A%20%20%20%20participant%20TeXRA%20UI%0A%20%20%20%20participant%20Agent%20Backend%0A%20%20%20%20participant%20LLM%20API%0A%0A%20%20%20%20User-%3E%3ETeXRA%20UI%3A%20Selects%20files%2C%20agent%2C%20instruction%2C%20model%0A%20%20%20%20User-%3E%3ETeXRA%20UI%3A%20Clicks%20Execute%0A%20%20%20%20TeXRA%20UI-%3E%3EAgent%20Backend%3A%20run(config)%0A%20%20%20%20Agent%20Backend-%3E%3EAgent%20Backend%3A%20Initialize%20(Load%20agent%20definition%2C%20read%20files)%0A%20%20%20%20Note%20over%20Agent%20Backend%3A%20Constructs%20prompt%20from%20systemPrompt%2C%20userPrefix%2C%20userRequest%20templates%20%2B%20User%20Input%0A%20%20%20%20Agent%20Backend-%3E%3ELLM%20API%3A%20Create%20Response%20(Round%200%20Prompt)%0A%20%20%20%20Note%20over%20LLM%20API%3A%20Processes%20request%20based%20on%20prompts%0A%20%20%20%20LLM%20API--%3E%3EAgent%20Backend%3A%20Response%20(Text%20%2B%20Usage%20%2B%20StopReason)%0A%20%20%20%20Agent%20Backend-%3E%3EAgent%20Backend%3A%20Process%20Response%20(Save%20*_r0_*%20output%2C%20check%20for%20continuation)%0A%20%20%20%20Agent%20Backend--%3E%3ETeXRA%20UI%3A%20Update%20ProgressBoard%20%2F%20Signal%20Completion%0A"})]),fallback:r(()=>e[13]||(e[13]=[o(" Loading... ")])),_:1})),e[16]||(e[16]=n("",19))])}const L=l(f,[["render",h]]);export{R as __pageData,L as default};
