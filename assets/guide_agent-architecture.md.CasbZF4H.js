import{_ as d,C as c,c as u,o as s,a6 as n,j as t,b as p,a as o,t as a,w as r,G as g,a7 as f}from"./chunks/framework.fkSL6LTs.js";const R=JSON.parse('{"title":"How TeXRA Agents Work: An Overview","description":"","frontmatter":{},"headers":[],"relativePath":"guide/agent-architecture.md","filePath":"guide/agent-architecture.md"}'),m={name:"guide/agent-architecture.md"};function h(i,e,A,y,T,b){const l=c("Mermaid");return s(),u("div",null,[e[15]||(e[15]=n('<h1 id="how-texra-agents-work-an-overview" tabindex="-1">How TeXRA Agents Work: An Overview <a class="header-anchor" href="#how-texra-agents-work-an-overview" aria-label="Permalink to &quot;How TeXRA Agents Work: An Overview&quot;">​</a></h1><p>At its core, a TeXRA agent is a recipe for instructing a Large Language Model (LLM) to perform a specific academic research task. This guide provides a high-level overview of how these agents are defined and how they execute your requests.</p><h2 id="agent-definition-files-yaml" tabindex="-1">Agent Definition Files (<code>.yaml</code>) <a class="header-anchor" href="#agent-definition-files-yaml" aria-label="Permalink to &quot;Agent Definition Files (`.yaml`)&quot;">​</a></h2><p>The core of TeXRA&#39;s agent definition lies in a combination of YAML for structure, Jinja2 for templating, and often XML within the prompts for guiding the LLM&#39;s output. Each agent&#39;s behavior is defined in a <code>.yaml</code> file located in the built-in or custom agent directories.</p><h2 id="understanding-the-yaml-structure" tabindex="-1">Understanding the YAML Structure <a class="header-anchor" href="#understanding-the-yaml-structure" aria-label="Permalink to &quot;Understanding the YAML Structure&quot;">​</a></h2><p>These <code>.yaml</code> files have two main parts (and thankfully, YAML is usually less prickly than XML or JSON):</p>',6)),t("ol",null,[e[9]||(e[9]=n('<li><strong><code>settings</code></strong>: Define general operational parameters. For example: <ul><li><code>agentType</code>: Is it a complex <code>CoT</code> (Chain of Thought) agent that &quot;thinks&quot; step-by-step, a simpler <code>direct</code> agent, or a <code>toolUse</code> agent designed to call model-integrated tools?</li><li><code>prefills</code>: Text the agent should automatically start its response with (e.g., <code>&lt;scratchpad&gt;</code>).</li><li><em>(Other settings control output format, inheritance, etc. See <a href="./configuration.html">Configuration</a> and <a href="./custom-agents.html">Custom Agents</a> for full details).</em></li></ul></li>',1)),t("li",null,[e[7]||(e[7]=t("strong",null,[t("code",null,"prompts")],-1)),e[8]||(e[8]=o(": Contain text templates that TeXRA fills with your specific context (input files, instructions) to guide the LLM at different stages: ")),t("ul",null,[e[4]||(e[4]=t("li",null,[t("code",null,"systemPrompt"),o(": Sets the overall role and high-level instructions for the LLM.")],-1)),t("li",null,[e[0]||(e[0]=t("code",null,"userPrefix",-1)),e[1]||(e[1]=o(": Provides the main context, including your input file(s) (available via e.g., ")),t("code",null,a(i.INPUT_CONTENT),1),e[2]||(e[2]=o(") and the specific instruction you typed in the UI (available via ")),t("code",null,a(i.INSTRUCTION),1),e[3]||(e[3]=o(")."))]),e[5]||(e[5]=t("li",null,[t("code",null,"userRequest"),o(": Asks the LLM to perform the initial task (Round 0). Often instructs the LLM to think within "),t("code",null,"<scratchpad>"),o(" tags and then output the main content wrapped within the XML tags defined by "),t("code",null,"settings.documentTag"),o(" (e.g., "),t("code",null,"<document>...</document>"),o(").")],-1)),e[6]||(e[6]=t("li",null,[t("code",null,"userReflect"),o(': Asks the LLM to review and improve its first response (Round 1, only used if "Reflect" is enabled).')],-1))])])]),t("p",null,[e[10]||(e[10]=o("_(Prompts use Jinja2 templating. For a detailed list of available variables like ")),t("code",null,a(i.INPUT_CONTENT),1),e[11]||(e[11]=o(" and how to use them, see the ")),e[12]||(e[12]=t("a",{href:"./custom-agents.html"},"Custom Agents",-1)),e[13]||(e[13]=o(" guide.)*"))]),e[16]||(e[16]=n('<div class="tip custom-block"><p class="custom-block-title">Transparency &amp; Customization</p><p>The prompts described above (<code>systemPrompt</code>, <code>userPrefix</code>, etc.) represent TeXRA&#39;s structured approach to guiding the LLM. This structured, template-based system means the agent&#39;s behavior is transparent and highly customizable through the <code>.yaml</code> file, not a hidden black box.</p></div><h2 id="basic-execution-flow" tabindex="-1">Basic Execution Flow <a class="header-anchor" href="#basic-execution-flow" aria-label="Permalink to &quot;Basic Execution Flow&quot;">​</a></h2><p>When you click &quot;Execute&quot; in the TeXRA UI, TeXRA uses the selected agent&#39;s definition (<code>.yaml</code>) and your UI inputs to interact with the chosen LLM:</p>',3)),(s(),p(f,null,{default:r(()=>[g(l,{id:"mermaid-83",class:"mermaid",graph:"sequenceDiagram%0A%20%20%20%20participant%20User%0A%20%20%20%20participant%20TeXRA%20UI%0A%20%20%20%20participant%20Agent%20Backend%0A%20%20%20%20participant%20LLM%20API%0A%0A%20%20%20%20User-%3E%3ETeXRA%20UI%3A%20Selects%20files%2C%20agent%2C%20instruction%2C%20model%0A%20%20%20%20User-%3E%3ETeXRA%20UI%3A%20Clicks%20Execute%0A%20%20%20%20TeXRA%20UI-%3E%3EAgent%20Backend%3A%20run(config)%0A%20%20%20%20Agent%20Backend-%3E%3EAgent%20Backend%3A%20Initialize%20(Load%20agent%20definition%2C%20read%20files)%0A%20%20%20%20Note%20over%20Agent%20Backend%3A%20Constructs%20prompt%20from%20systemPrompt%2C%20userPrefix%2C%20userRequest%20templates%20%2B%20User%20Input%0A%20%20%20%20Agent%20Backend-%3E%3ELLM%20API%3A%20Create%20Response%20(Round%200%20Prompt)%0A%20%20%20%20Note%20over%20LLM%20API%3A%20Processes%20request%20based%20on%20prompts%0A%20%20%20%20LLM%20API--%3E%3EAgent%20Backend%3A%20Response%20(Text%20%2B%20Usage%20%2B%20StopReason)%0A%20%20%20%20Agent%20Backend-%3E%3EAgent%20Backend%3A%20Process%20Response%20(Save%20*_r0_*%20output%2C%20check%20for%20continuation)%0A%20%20%20%20Agent%20Backend--%3E%3ETeXRA%20UI%3A%20Update%20ProgressBoard%20%2F%20Signal%20Completion%0A"})]),fallback:r(()=>e[14]||(e[14]=[o(" Loading... ")])),_:1})),e[17]||(e[17]=n('<p><strong>Key Stages:</strong></p><ol><li><strong>Initialization:</strong> TeXRA loads the agent definition and reads the files you selected.</li><li><strong>Prompt Construction:</strong> It combines the agent&#39;s <code>systemPrompt</code>, <code>userPrefix</code> (filled with your files and instruction), and <code>userRequest</code> templates into a full prompt for the LLM.</li><li><strong>LLM Interaction (Round 0):</strong> TeXRA sends the prompt to the selected LLM API. The LLM generates a response, typically including reasoning (<code>&lt;scratchpad&gt;</code>) and the final answer wrapped in XML tags (e.g., <code>&lt;document&gt;...&lt;/document&gt;</code>).</li><li><strong>Processing:</strong> TeXRA saves the raw LLM response (often as an <code>.xml</code> file internally). It then parses this file, extracts the content from the primary XML tag (defined by <code>settings.documentTag</code>), and saves <em>that extracted content</em> to the final output file (e.g., <code>filename_agent_r0_model.tex</code>). You can monitor this in the <a href="./progress-board.html">ProgressBoard</a>. For LaTeX files, TeXRA can also automatically generate a <code>latexdiff</code> file comparing the output to the input, enhancing observability. See the <a href="./latex-diff.html">LaTeX Diff guide</a> for details.</li></ol><p><strong>Continuation Handling:</strong> If the LLM response gets cut off due to output token limits before generating the required <code>endTag</code>, TeXRA automatically sends a continuation prompt. This prompt asks the model to resume generating exactly where it left off, ensuring complete outputs even for very long tasks. This happens seamlessly within a processing round.</p><h3 id="prompt-composition-and-message-flow" tabindex="-1">Prompt Composition and Message Flow <a class="header-anchor" href="#prompt-composition-and-message-flow" aria-label="Permalink to &quot;Prompt Composition and Message Flow&quot;">​</a></h3><p>TeXRA constructs the conversation by merging your agent&#39;s <code>systemPrompt</code>, the context-filled <code>userPrefix</code>, and the <code>userRequest</code>. Depending on settings, the extension may insert additional messages in between—for example the output of <code>texcount</code> when you enable <strong>Attach TeX Count</strong>, or encoded images and audio files selected in the file panel. The sequence is not a fixed &quot;system–user–system&quot; pattern: attachments or tool results can be inserted at any point before the LLM generates a single response containing <code>&lt;scratchpad&gt;</code> reasoning followed by the XML-wrapped output defined by <code>settings.documentTag</code>.</p><h3 id="promptbuilder-utility" tabindex="-1">PromptBuilder utility <a class="header-anchor" href="#promptbuilder-utility" aria-label="Permalink to &quot;PromptBuilder utility&quot;">​</a></h3><p>Internally, TeXRA now assembles these prompt segments through the <code>PromptBuilder</code> helper. The builder collects the agent&#39;s templates and rendered variables once and exposes focused methods:</p><ul><li><code>buildInitialPrompts()</code> returns the trio of system, prefix, and request messages used for round 0.</li><li><code>buildReflectPrompt(round)</code> renders the appropriate <code>userReflect</code> template for a given reflection round.</li><li><code>getPrefill(round)</code> provides the prefill seed that is streamed to the assistant before each model turn.</li></ul><p>Agents that inherit from <code>BaseReflectionAgent</code> can override the protected <code>getPromptBuilder()</code> hook to supply a subclassed builder. This makes it easy to add new phases (e.g., a planning stage) or to customize how prefills are computed without rewriting the round-processing logic. When you introduce a specialised agent, create a derived <code>PromptBuilder</code> that extends the base implementation, override or add the necessary methods, and return it from your agent&#39;s <code>getPromptBuilder()</code> override so the lifecycle automatically uses your custom prompts.</p><p><strong>Optional Reflection (Round 1):</strong></p><p>If you enable the &quot;Reflect&quot; option in the Tool Config section of the UI, TeXRA performs an additional step after Round 0 finishes successfully:</p><ol><li><strong>Reflection Prompt:</strong> It uses the agent&#39;s <code>userReflect</code> prompt template to ask the LLM to critique and improve its own Round 0 output (which is included in the conversation history).</li><li><strong>LLM Interaction (Round 1):</strong> The LLM generates a revised response.</li><li><strong>Processing:</strong> TeXRA saves this refined output to a separate file (e.g., <code>filename_agent_r1_model.ext</code>).</li></ol><p>This basic flow, potentially with the reflection step, allows TeXRA agents to perform targeted tasks based on their specific definitions and your instructions. For concrete examples of built-in agents, see the <a href="./built-in-agents.html">Built-in Agent Reference</a>.</p><div class="warning custom-block"><p class="custom-block-title">Potential XML Issues</p><p>Occasionally, LLMs might generate slightly malformed XML (e.g., missing closing tags), especially with very long or complex outputs. If TeXRA fails to extract content from an agent&#39;s output (<code>_r0_*.xml</code> or <code>_r1_*.xml</code> file), you might need to manually inspect the <code>.xml</code> file and correct any structural errors (like adding a missing <code>&lt;/document&gt;</code> tag) before TeXRA can process it correctly. See the <a href="./../reference/troubleshooting.html#output-file-corruption">Troubleshooting guide</a> for more details.</p></div><h3 id="reflection" tabindex="-1">Reflection <a class="header-anchor" href="#reflection" aria-label="Permalink to &quot;Reflection&quot;">​</a></h3><p>After generating an initial output (Round 0), TeXRA agents with reflection enabled evaluate and refine their work (Round 1):</p><div class="reflection-pdf-viewer"><div class="pdf-tabs"><button type="button" class="pdf-tab active" data-pdf="/examples/draft_polish_r0_gemini25p_diff.pdf">Original vs. Round 0</button><button type="button" class="pdf-tab" data-pdf="/examples/draft_polish_r1_gemini25p_diff.pdf">Original vs. Round 1</button><button type="button" class="pdf-tab" data-pdf="/examples/draft_polish_r1_gemini25p_diffr1r0.pdf">Round 0 vs. Round 1</button></div><iframe src="/examples/draft_polish_r1_gemini25p_diffr1r0.pdf" id="pdf-frame" class="reflection-pdf-frame"></iframe><a href="/examples/draft_polish_r1_gemini25p_diffr1r0.pdf" target="_blank" id="pdf-link" class="reflection-pdf-link">View full example</a></div><div class="reflection-legend"><div class="legend-item"><span class="del">Red strikethrough</span>: Round 0 content revised in Round 1</div><div class="legend-item"><span class="add">Blue underlined</span>: New/improved content added in Round 1</div></div>',18))])}const L=d(m,[["render",h]]);export{R as __pageData,L as default};
