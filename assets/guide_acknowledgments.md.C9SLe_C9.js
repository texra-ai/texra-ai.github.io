import{_ as a,c as o,o as t,a6 as n}from"./chunks/framework.fkSL6LTs.js";const u=JSON.parse('{"title":"Acknowledgments & Citation","description":"","frontmatter":{},"headers":[],"relativePath":"guide/acknowledgments.md","filePath":"guide/acknowledgments.md"}'),r={name:"guide/acknowledgments.md"};function i(s,e,l,c,p,d){return t(),o("div",null,e[0]||(e[0]=[n('<h1 id="acknowledgments-citation" tabindex="-1">Acknowledgments &amp; Citation <a class="header-anchor" href="#acknowledgments-citation" aria-label="Permalink to &quot;Acknowledgments &amp; Citation&quot;">​</a></h1><h2 id="acknowledging-texra" tabindex="-1">Acknowledging TeXRA <a class="header-anchor" href="#acknowledging-texra" aria-label="Permalink to &quot;Acknowledging TeXRA&quot;">​</a></h2><p>We are thrilled if TeXRA proves useful for your academic research! While not required, if TeXRA played a significant role in your work—particularly if used as part of a study or evaluation involving LLMs or academic research tools—we would greatly appreciate an acknowledgment or citation if possible.</p><p>As the project evolves, we will provide a preferred citation format (e.g., a white paper or software citation). Please refer to the <a href="https://github.com/texra-ai/texra-issues" target="_blank" rel="noreferrer">TeXRA GitHub repository</a> or the <a href="https://texra.ai" target="_blank" rel="noreferrer">texra.ai</a> website for future citation details.</p><p>Your feedback and potential acknowledgments help support the continued development and improvement of TeXRA. Thank you for using it!</p><h3 id="supporting-texra" tabindex="-1">Supporting TeXRA <a class="header-anchor" href="#supporting-texra" aria-label="Permalink to &quot;Supporting TeXRA&quot;">​</a></h3><p>If TeXRA helps you publish faster, graduate sooner, or simply reduces your LaTeX-induced stress levels, consider supporting its development:</p><ul><li><a href="https://github.com/sponsors/YOUR_USERNAME" target="_blank" rel="noreferrer"><strong>Sponsor on GitHub</strong></a></li><li><a href="https://www.buymeacoffee.com/YOUR_USERNAME" target="_blank" rel="noreferrer"><strong>Buy Me a Coffee</strong></a></li></ul><h2 id="conceptual-background-references" tabindex="-1">Conceptual Background &amp; References <a class="header-anchor" href="#conceptual-background-references" aria-label="Permalink to &quot;Conceptual Background &amp; References&quot;">​</a></h2><p>TeXRA&#39;s design draws inspiration from several key concepts in AI and software development:</p><ul><li><strong>Agentic Workflows &amp; Tool Use [1]:</strong> The core idea involves AI agents executing tasks augmented by specialized tools (e.g., <code>texcount</code>). This allows LLMs to leverage external capabilities for tasks requiring precision or specific knowledge beyond their training data.</li><li><strong>Chain-of-Thought (CoT) Reasoning [2]:</strong> For complex agents, TeXRA employs techniques inspired by Chain-of-Thought prompting, encouraging models to &quot;think step-by-step&quot; (often visible in the <code>&lt;scratchpad&gt;</code> sections of logs) before producing a final output.</li><li><strong>Reflection &amp; Action [3, 4]:</strong> The automatic reflection passes, combined with the agent&#39;s ability to act (edit text, use tools), draw inspiration from frameworks like ReAct and Reflexion, allowing iterative refinement based on self-critique or environmental feedback.</li><li><strong>Structured Prompting (YAML + Jinja):</strong> The use of YAML for structure and Jinja for templating within prompts allows for complex logic, dynamic content injection, and better maintainability, drawing inspiration from approaches seen in libraries like <a href="https://github.com/character-ai/prompt-poet" target="_blank" rel="noreferrer">Prompt Poet</a>. The support for inheritance and modularity allows for a more flexible and reusable prompt design.</li><li><strong>Scientific discovery workflows [5]:</strong> TeXRA&#39;s focus on reproducible, domain-aware assistance aligns with emerging work on language agents that support theoretical and computational physics research.</li></ul><p>We believe combining these concepts provides a robust and adaptable platform for AI-powered academic writing assistance.</p><h3 id="references" tabindex="-1">References <a class="header-anchor" href="#references" aria-label="Permalink to &quot;References&quot;">​</a></h3><p>[1] Schick, T., Dwivedi-Yu, J., Dessì, R., Raileanu, R., Lomeli, M., Zettlemoyer, L., Cancedda, N., &amp; Scialom, T. (2023). Toolformer: Language Models Can Teach Themselves to Use Tools. <em>arXiv preprint arXiv:2302.04761</em>.</p><p>[2] Wei, J., Wang, X., Schuurmans, D., Bosma, M., Ichter, B., Xia, F., Chi, E., Le, Q., &amp; Zhou, D. (2022). Chain-of-Thought Prompting Elicits Reasoning in Large Language Models. <em>Advances in Neural Information Processing Systems (NeurIPS)</em>, 35, 24824–24837.</p><p>[3] Yao, S., Zhao, J., Yu, D., Du, N., Shafran, I., Narasimhan, K., &amp; Cao, Y. (2023). ReAct: Synergizing Reasoning and Acting in Language Models. <em>International Conference on Learning Representations (ICLR)</em>.</p><p>[4] Shinn, N., Cassano, F., Gopinath, A., Narasimhan, K., &amp; Yao, S. (2023). Reflexion: Language Agents with Verbal Reinforcement Learning. In <em>Advances in Neural Information Processing Systems 36 (NeurIPS 2023)</em>.</p><p>[5] Lu, S., Jin, Z., Zhang, T. J., Kos, P., Cirac, J. I., &amp; Schölkopf, B. (2025). Can Theoretical Physics Research Benefit from Language Agents? <em>arXiv preprint arXiv:2506.06214</em>.</p>',18)]))}const m=a(r,[["render",i]]);export{u as __pageData,m as default};
