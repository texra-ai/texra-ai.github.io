import{_ as e,c as l,o as s,a6 as i}from"./chunks/framework.fkSL6LTs.js";const c=JSON.parse('{"title":"AI Models","description":"","frontmatter":{},"headers":[],"relativePath":"guide/models.md","filePath":"guide/models.md"}'),a={name:"guide/models.md"};function n(d,t,o,r,h,g){return s(),l("div",null,t[0]||(t[0]=[i(`<h1 id="ai-models" tabindex="-1">AI Models <a class="header-anchor" href="#ai-models" aria-label="Permalink to &quot;AI Models&quot;">​</a></h1><p>TeXRA supports a variety of language models from different providers, allowing you to choose the best fit for your task&#39;s complexity, required speed, and budget (think of it as choosing your research assistant&#39;s brain!). This guide provides an overview of the models available by default.</p><h2 id="model-providers-overview" tabindex="-1">Model Providers Overview <a class="header-anchor" href="#model-providers-overview" aria-label="Permalink to &quot;Model Providers Overview&quot;">​</a></h2><p>TeXRA primarily integrates with models from:</p><ol><li><strong>Anthropic</strong> (Claude family)</li><li><strong>OpenAI</strong> (GPT and O-series families)</li><li><strong>Google</strong> (Gemini family)</li><li><strong>Other Providers</strong> (via OpenRouter, including Grok, DeepSeek)</li></ol><p>You can select the desired model from the dropdown list in the TeXRA UI. Hovering over an option shows its provider, context window, and estimated cost.</p><h2 id="default-model-selection" tabindex="-1">Default Model Selection <a class="header-anchor" href="#default-model-selection" aria-label="Permalink to &quot;Default Model Selection&quot;">​</a></h2><p>Here&#39;s a quick comparison of the models available by default in TeXRA:</p><h3 id="anthropic-models" tabindex="-1">Anthropic Models <a class="header-anchor" href="#anthropic-models" aria-label="Permalink to &quot;Anthropic Models&quot;">​</a></h3><p>Known for strong instruction following and context handling.</p><table tabindex="0"><thead><tr><th style="text-align:left;">Model ID</th><th style="text-align:left;">Key Strength / Use Case</th><th style="text-align:left;">Relative Cost</th><th style="text-align:left;">Relative Speed</th><th style="text-align:left;">Notes</th></tr></thead><tbody><tr><td style="text-align:left;"><code>opus41T</code></td><td style="text-align:left;">Latest Opus with explicit reasoning steps</td><td style="text-align:left;">$$$$</td><td style="text-align:left;">Slow</td><td style="text-align:left;">Claude 4.1 Opus with thinking</td></tr><tr><td style="text-align:left;"><code>opus41</code></td><td style="text-align:left;">Latest high quality, complex tasks</td><td style="text-align:left;">$$$$</td><td style="text-align:left;">Slow</td><td style="text-align:left;">Claude 4.1 Opus</td></tr><tr><td style="text-align:left;"><code>opus4T</code></td><td style="text-align:left;">Opus 4 with explicit reasoning steps</td><td style="text-align:left;">$$$$</td><td style="text-align:left;">Slow</td><td style="text-align:left;">Claude 4 Opus with thinking</td></tr><tr><td style="text-align:left;"><code>opus4</code></td><td style="text-align:left;">Opus 4 high quality, complex tasks</td><td style="text-align:left;">$$$$</td><td style="text-align:left;">Slow</td><td style="text-align:left;">Claude 4 Opus</td></tr><tr><td style="text-align:left;"><code>sonnet45T</code></td><td style="text-align:left;">Latest Sonnet with explicit reasoning steps</td><td style="text-align:left;">$$$</td><td style="text-align:left;">Medium</td><td style="text-align:left;">Claude 4.5 Sonnet with thinking</td></tr><tr><td style="text-align:left;"><code>sonnet45</code></td><td style="text-align:left;">Latest strong all-rounder</td><td style="text-align:left;">$$$</td><td style="text-align:left;">Medium</td><td style="text-align:left;">Claude 4.5 Sonnet</td></tr><tr><td style="text-align:left;"><code>sonnet4T</code></td><td style="text-align:left;">Sonnet 4 with explicit reasoning steps</td><td style="text-align:left;">$$$</td><td style="text-align:left;">Medium</td><td style="text-align:left;">Claude 4 Sonnet with thinking</td></tr><tr><td style="text-align:left;"><code>sonnet4</code></td><td style="text-align:left;">Sonnet 4 strong all-rounder</td><td style="text-align:left;">$$$</td><td style="text-align:left;">Medium</td><td style="text-align:left;">Claude 4 Sonnet</td></tr><tr><td style="text-align:left;"><code>sonnet37T</code></td><td style="text-align:left;"><code>sonnet37</code> with explicit reasoning steps</td><td style="text-align:left;">$$$</td><td style="text-align:left;">Medium</td><td style="text-align:left;">Good for math, complex logic</td></tr><tr><td style="text-align:left;"><code>sonnet37</code></td><td style="text-align:left;">Strong all-rounder, good context</td><td style="text-align:left;">$$$</td><td style="text-align:left;">Medium</td><td style="text-align:left;"></td></tr><tr><td style="text-align:left;"><code>sonnet35</code></td><td style="text-align:left;">Good balance of quality/cost (older Sonnet)</td><td style="text-align:left;">$$$</td><td style="text-align:left;">Medium</td><td style="text-align:left;"></td></tr><tr><td style="text-align:left;"><code>haiku45T</code></td><td style="text-align:left;">Fast Claude 4.5 with explicit reasoning</td><td style="text-align:left;">$$</td><td style="text-align:left;">Fast</td><td style="text-align:left;">Claude 4.5 Haiku with thinking</td></tr><tr><td style="text-align:left;"><code>haiku45</code></td><td style="text-align:left;">Fast Claude 4.5 responses</td><td style="text-align:left;">$$</td><td style="text-align:left;">Fast</td><td style="text-align:left;">Claude 4.5 Haiku</td></tr></tbody></table><h4 id="sonnet-4-4-5-1m-context-beta" tabindex="-1">Sonnet 4 / 4.5 1M Context (Beta) <a class="header-anchor" href="#sonnet-4-4-5-1m-context-beta" aria-label="Permalink to &quot;Sonnet 4 / 4.5 1M Context (Beta)&quot;">​</a></h4><p>To experiment with Anthropic&#39;s 1M-token context window for Sonnet 4 or 4.5, enable <code>&quot;texra.model.useAnthropic1MBeta&quot;: true</code> in VS Code settings. The extension attaches the <code>context-1m-2025-08-07</code> beta header for these requests. Only Sonnet 4-family models support this beta, and TeXRA still enforces the tier‑4 limit of 200 K tokens.</p><h3 id="openai-models" tabindex="-1">OpenAI Models <a class="header-anchor" href="#openai-models" aria-label="Permalink to &quot;OpenAI Models&quot;">​</a></h3><p>Known for strong reasoning and creative capabilities.</p><table tabindex="0"><thead><tr><th style="text-align:left;">Model ID</th><th style="text-align:left;">Key Strength / Use Case</th><th style="text-align:left;">Relative Cost</th><th style="text-align:left;">Relative Speed</th><th style="text-align:left;">Notes</th></tr></thead><tbody><tr><td style="text-align:left;"><code>o1</code></td><td style="text-align:left;">Advanced reasoning, math, figures</td><td style="text-align:left;">$$$$</td><td style="text-align:left;">Slow</td><td style="text-align:left;">Explicit reasoning</td></tr><tr><td style="text-align:left;"><code>gpt45</code></td><td style="text-align:left;">High quality, vision (Preview)</td><td style="text-align:left;">$$$$</td><td style="text-align:left;">Medium</td><td style="text-align:left;"></td></tr><tr><td style="text-align:left;"><code>gpt5pro</code></td><td style="text-align:left;">Premium reasoning &amp; coding</td><td style="text-align:left;">$$$$</td><td style="text-align:left;">Slow</td><td style="text-align:left;">400k ctx, 272k max output</td></tr><tr><td style="text-align:left;"><code>gpt5</code></td><td style="text-align:left;">Flagship reasoning &amp; coding</td><td style="text-align:left;">$$$</td><td style="text-align:left;">Medium</td><td style="text-align:left;">400k context</td></tr><tr><td style="text-align:left;"><code>gpt5-</code></td><td style="text-align:left;">Flagship mini, fast</td><td style="text-align:left;">$$</td><td style="text-align:left;">Fast</td><td style="text-align:left;">400k context, mini</td></tr><tr><td style="text-align:left;"><code>gpt5--</code></td><td style="text-align:left;">Flagship nano, fastest</td><td style="text-align:left;">$</td><td style="text-align:left;">Very Fast</td><td style="text-align:left;">400k context, nano</td></tr><tr><td style="text-align:left;"><code>gpt41</code></td><td style="text-align:left;">Long-context vision, powerful</td><td style="text-align:left;">$$$</td><td style="text-align:left;">Medium</td><td style="text-align:left;">1M tokens context</td></tr><tr><td style="text-align:left;"><code>gpt41-</code></td><td style="text-align:left;">Long-context vision, cost-effective</td><td style="text-align:left;">$$</td><td style="text-align:left;">Medium</td><td style="text-align:left;">1M tokens context, mini</td></tr><tr><td style="text-align:left;"><code>gpt41--</code></td><td style="text-align:left;">Long-context vision, cheapest</td><td style="text-align:left;">$</td><td style="text-align:left;">Medium</td><td style="text-align:left;">1M tokens context, nano</td></tr><tr><td style="text-align:left;"><code>gpt4o</code></td><td style="text-align:left;">Strong all-rounder, vision</td><td style="text-align:left;">$$$</td><td style="text-align:left;">Medium</td><td style="text-align:left;">Good default choice</td></tr><tr><td style="text-align:left;"><code>gpt4ol</code></td><td style="text-align:left;">Latest <code>gpt4o</code>, potentially better</td><td style="text-align:left;">$$$</td><td style="text-align:left;">Medium</td><td style="text-align:left;"></td></tr><tr><td style="text-align:left;"><code>o3</code></td><td style="text-align:left;">Coding, tool calling</td><td style="text-align:left;">$$$</td><td style="text-align:left;">Medium</td><td style="text-align:left;"></td></tr><tr><td style="text-align:left;"><code>o3pro</code></td><td style="text-align:left;">Reliable answers, heavy compute</td><td style="text-align:left;">$$$$</td><td style="text-align:left;">Slow</td><td style="text-align:left;"><code>o3-pro</code></td></tr><tr><td style="text-align:left;"><code>o3-</code></td><td style="text-align:left;">Fast reasoning</td><td style="text-align:left;">$$$</td><td style="text-align:left;">Fast</td><td style="text-align:left;"><code>o3-mini</code></td></tr><tr><td style="text-align:left;"><code>o1-</code></td><td style="text-align:left;">Fast reasoning (smaller <code>o1</code>)</td><td style="text-align:left;">$$$</td><td style="text-align:left;">Fast</td><td style="text-align:left;"><code>o1-mini</code></td></tr><tr><td style="text-align:left;"><code>gptoss</code></td><td style="text-align:left;">Open-weight reasoning, large context</td><td style="text-align:left;">$$</td><td style="text-align:left;">Medium</td><td style="text-align:left;"><code>gpt-oss-120b</code> (OpenRouter only)</td></tr><tr><td style="text-align:left;"><code>gptoss-</code></td><td style="text-align:left;">Open-weight reasoning, cost-effective</td><td style="text-align:left;">$</td><td style="text-align:left;">Fast</td><td style="text-align:left;"><code>gpt-oss-20b</code> (OpenRouter only)</td></tr></tbody></table><blockquote><p><strong>Note:</strong> GPT-5 and GPT-5 Pro reasoning summaries require additional account verification. TeXRA disables them by default—enable <code>&quot;texra.model.gpt5ReasoningSummary&quot;: true</code> if your account supports this feature.</p></blockquote><h3 id="google-models" tabindex="-1">Google Models <a class="header-anchor" href="#google-models" aria-label="Permalink to &quot;Google Models&quot;">​</a></h3><p>Known for large context windows, multimodality, and speed/cost efficiency.</p><table tabindex="0"><thead><tr><th style="text-align:left;">Model ID</th><th style="text-align:left;">Key Strength / Use Case</th><th style="text-align:left;">Relative Cost</th><th style="text-align:left;">Relative Speed</th><th style="text-align:left;">Notes</th></tr></thead><tbody><tr><td style="text-align:left;"><code>gemini25p</code></td><td style="text-align:left;">Strong reasoning, vision, large context</td><td style="text-align:left;">$$$</td><td style="text-align:left;">Medium</td><td style="text-align:left;">Latest Pro model</td></tr><tr><td style="text-align:left;"><code>gemini2p</code></td><td style="text-align:left;">Good reasoning, vision, very large context</td><td style="text-align:left;">$$</td><td style="text-align:left;">Medium</td><td style="text-align:left;"></td></tr><tr><td style="text-align:left;"><code>gemini25f</code></td><td style="text-align:left;">Fast reasoning, large context</td><td style="text-align:left;">$$</td><td style="text-align:left;">Fast</td><td style="text-align:left;">Latest Flash model</td></tr><tr><td style="text-align:left;"><code>gemini2fT</code></td><td style="text-align:left;"><code>gemini2f</code> with explicit reasoning steps</td><td style="text-align:left;">$</td><td style="text-align:left;">Fast</td><td style="text-align:left;"></td></tr><tr><td style="text-align:left;"><code>gemini2f</code></td><td style="text-align:left;">Fastest, most cost-effective, vision</td><td style="text-align:left;">$</td><td style="text-align:left;">Very Fast</td><td style="text-align:left;">Good for simple tasks, native PDF/audio</td></tr></tbody></table><h3 id="deepseek-models" tabindex="-1">DeepSeek Models <a class="header-anchor" href="#deepseek-models" aria-label="Permalink to &quot;DeepSeek Models&quot;">​</a></h3><p>Strong technical and coding performance, cost-effective. DeepSeek&#39;s API now supports function calling so agents can use external tools during a run.</p><table tabindex="0"><thead><tr><th style="text-align:left;">Model ID</th><th style="text-align:left;">Key Strength / Use Case</th><th style="text-align:left;">Relative Cost</th><th style="text-align:left;">Relative Speed</th><th style="text-align:left;">Notes</th></tr></thead><tbody><tr><td style="text-align:left;"><code>dsv3</code></td><td style="text-align:left;">Good coding &amp; general tasks</td><td style="text-align:left;">$</td><td style="text-align:left;">Fast</td><td style="text-align:left;">DeepSeek V3 Chat</td></tr><tr><td style="text-align:left;"><code>dsr1</code></td><td style="text-align:left;">Advanced reasoning</td><td style="text-align:left;">$$</td><td style="text-align:left;">Medium</td><td style="text-align:left;">DeepSeek R1</td></tr></tbody></table><h3 id="moonshot-kimi-models" tabindex="-1">Moonshot Kimi Models <a class="header-anchor" href="#moonshot-kimi-models" aria-label="Permalink to &quot;Moonshot Kimi Models&quot;">​</a></h3><p>High context models from Moonshot, suitable for complex reasoning and large documents.</p><p><strong>Kimi K2</strong> is Moonshot&#39;s open-source 1T‑parameter MoE model (32B active). It excels at coding and agentic tasks but currently lacks multimodal and thought-mode support. The 0905 preview offers a 256k context window, and a high-speed turbo variant is also available.</p><table tabindex="0"><thead><tr><th style="text-align:left;">Model ID</th><th style="text-align:left;">Key Strength / Use Case</th><th style="text-align:left;">Relative Cost</th><th style="text-align:left;">Relative Speed</th><th style="text-align:left;">Notes</th></tr></thead><tbody><tr><td style="text-align:left;"><code>kimit</code></td><td style="text-align:left;">Detailed reasoning with vision</td><td style="text-align:left;">$$$</td><td style="text-align:left;">Medium</td><td style="text-align:left;">Kimi Thinking Preview</td></tr><tr><td style="text-align:left;"><code>kimi</code></td><td style="text-align:left;">Large context, general tasks</td><td style="text-align:left;">$$</td><td style="text-align:left;">Medium</td><td style="text-align:left;">128k context</td></tr><tr><td style="text-align:left;"><code>kimiv</code></td><td style="text-align:left;">Vision-enabled variant</td><td style="text-align:left;">$$</td><td style="text-align:left;">Medium</td><td style="text-align:left;">128k context, vision</td></tr><tr><td style="text-align:left;"><code>kimi2</code></td><td style="text-align:left;">Agent tasks, 256k context</td><td style="text-align:left;">$$$</td><td style="text-align:left;">Medium</td><td style="text-align:left;">Kimi K2 0905 Preview (<code>moonshotai/kimi-k2-0905</code>)</td></tr><tr><td style="text-align:left;"><code>kimi2turbo</code></td><td style="text-align:left;">Fast agent tasks</td><td style="text-align:left;">$$$$</td><td style="text-align:left;">Very Fast</td><td style="text-align:left;">Kimi K2 Turbo Preview (<code>moonshotai/kimi-k2-turbo</code>)</td></tr></tbody></table><p>The earlier Kimi K2 0711 model remains available on OpenRouter as <code>moonshotai/kimi-k2</code>.</p><p>Additional resources: <a href="https://platform.moonshot.ai" target="_blank" rel="noreferrer">API</a> – $0.15/million input tokens (cache hit), $0.60/million input tokens (cache miss), $2.50/million output tokens. <a href="https://moonshotai.github.io/Kimi-K2/" target="_blank" rel="noreferrer">Tech blog</a>, <a href="https://huggingface.co/moonshotai" target="_blank" rel="noreferrer">Weights &amp; code</a>, <a href="https://github.com/MoonshotAI/Kimi-K2" target="_blank" rel="noreferrer">GitHub</a>.</p><h3 id="dashscope-qwen-models" tabindex="-1">DashScope Qwen Models <a class="header-anchor" href="#dashscope-qwen-models" aria-label="Permalink to &quot;DashScope Qwen Models&quot;">​</a></h3><p>Cost-effective models from Alibaba with strong multilingual capabilities.</p><table tabindex="0"><thead><tr><th style="text-align:left;">Model ID</th><th style="text-align:left;">Key Strength / Use Case</th><th style="text-align:left;">Relative Cost</th><th style="text-align:left;">Relative Speed</th><th style="text-align:left;">Notes</th></tr></thead><tbody><tr><td style="text-align:left;"><code>qwen3max</code></td><td style="text-align:left;">Flagship coding agent, 262k ctx</td><td style="text-align:left;">$$$</td><td style="text-align:left;">Medium</td><td style="text-align:left;">Qwen3-Max latest, no thinking mode</td></tr><tr><td style="text-align:left;"><code>qwenplus</code></td><td style="text-align:left;">Hybrid thinking, 1M ctx + tool use</td><td style="text-align:left;">$$</td><td style="text-align:left;">Medium</td><td style="text-align:left;">Qwen Plus latest, enable_thinking</td></tr><tr><td style="text-align:left;"><code>qwenturbo</code></td><td style="text-align:left;">Fast responses with optional thinking mode</td><td style="text-align:left;">$</td><td style="text-align:left;">Fast</td><td style="text-align:left;">Qwen Turbo, enable_thinking</td></tr></tbody></table><p>Deep thinking models first stream their reasoning before the final answer. <code>qwenplus</code> and <code>qwenturbo</code> support this mode. Pass <code>enable_thinking: true</code> in the request body to turn it on; commercial tiers disable it by default. <code>qwen3max</code> always runs in non-thinking mode.</p><div class="language-python vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">from</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> openai </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">import</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> OpenAI</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">import</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> os</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">client </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> OpenAI(</span></span>
<span class="line"><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">    api_key</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">os.getenv(</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;DASHSCOPE_API_KEY&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">),</span></span>
<span class="line"><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">    base_url</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;https://dashscope-intl.aliyuncs.com/compatible-mode/v1&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">,</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">resp </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> client.chat.completions.create(</span></span>
<span class="line"><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">    model</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;qwen-plus-latest&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">,</span></span>
<span class="line"><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">    messages</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">[{</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;role&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;user&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;content&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;Who are you?&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">}],</span></span>
<span class="line"><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">    extra_body</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">{</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;enable_thinking&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">},</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">print</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(resp.choices[</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">].message.reasoning_content)</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">print</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(resp.choices[</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">].message.content)</span></span></code></pre></div><p>Use the <code>thinking_budget</code> parameter to cap how many tokens the reasoning step can consume.</p><h3 id="copilot-models" tabindex="-1">Copilot Models <a class="header-anchor" href="#copilot-models" aria-label="Permalink to &quot;Copilot Models&quot;">​</a></h3><p>GitHub Copilot models are available through VS Code&#39;s built-in Language Model API. These models require user consent and sign in to GitHub Copilot.</p><table tabindex="0"><thead><tr><th style="text-align:left;">Model ID</th><th style="text-align:left;">Key Strength / Use Case</th><th style="text-align:left;">Relative Cost</th><th style="text-align:left;">Relative Speed</th><th style="text-align:left;">Notes</th></tr></thead><tbody><tr><td style="text-align:left;"><code>copilot4o</code></td><td style="text-align:left;">Strong all-rounder, vision</td><td style="text-align:left;">$$</td><td style="text-align:left;">Medium</td><td style="text-align:left;">Uses GPT-4o backend</td></tr></tbody></table><h3 id="grok-xai-models" tabindex="-1">Grok / xAI Models <a class="header-anchor" href="#grok-xai-models" aria-label="Permalink to &quot;Grok / xAI Models&quot;">​</a></h3><p>Large context models from xAI.</p><table tabindex="0"><thead><tr><th style="text-align:left;">Model ID</th><th style="text-align:left;">Key Strength / Use Case</th><th style="text-align:left;">Relative Cost</th><th style="text-align:left;">Relative Speed</th><th style="text-align:left;">Notes</th></tr></thead><tbody><tr><td style="text-align:left;"><code>grok4</code></td><td style="text-align:left;">Very large context, strong reasoning</td><td style="text-align:left;">$$$</td><td style="text-align:left;">Medium</td><td style="text-align:left;">xAI Grok 4</td></tr><tr><td style="text-align:left;"><code>grok3</code></td><td style="text-align:left;">Large context, alternative reasoning</td><td style="text-align:left;">$$$</td><td style="text-align:left;">Medium</td><td style="text-align:left;">xAI Grok 3</td></tr><tr><td style="text-align:left;"><code>grok3-</code></td><td style="text-align:left;">Faster Grok 3 (mini)</td><td style="text-align:left;">$$</td><td style="text-align:left;">Fast</td><td style="text-align:left;">xAI Grok 3 Mini</td></tr></tbody></table><h3 id="other-models-available-primarily-via-openrouter" tabindex="-1">Other Models (Available Primarily via OpenRouter) <a class="header-anchor" href="#other-models-available-primarily-via-openrouter" aria-label="Permalink to &quot;Other Models (Available Primarily via OpenRouter)&quot;">​</a></h3><p>These models are generally accessed by enabling OpenRouter in settings.</p><table tabindex="0"><thead><tr><th style="text-align:left;">Model ID</th><th style="text-align:left;">Key Strength / Use Case</th><th style="text-align:left;">Provider</th><th style="text-align:left;">Relative Cost</th><th style="text-align:left;">Relative Speed</th></tr></thead><tbody><tr><td style="text-align:left;"><code>llama31</code></td><td style="text-align:left;">Strong open model, large context</td><td style="text-align:left;">Meta</td><td style="text-align:left;">$$$</td><td style="text-align:left;">Medium</td></tr><tr><td style="text-align:left;"><code>qvq-72b</code></td><td style="text-align:left;">Strong multi-lingual</td><td style="text-align:left;">Qwen/Alibaba</td><td style="text-align:left;">$$</td><td style="text-align:left;">Medium</td></tr></tbody></table><p><em>Relative Cost/Speed are estimates: $ = Low/Fast, $$$$ = High/Slow.</em></p><h2 id="choosing-the-right-model" tabindex="-1">Choosing the Right Model <a class="header-anchor" href="#choosing-the-right-model" aria-label="Permalink to &quot;Choosing the Right Model&quot;">​</a></h2><p>Consider these factors:</p><ul><li><strong>Task Complexity</strong>: Simple corrections might only need a <code>$</code>/Fast model (<code>gemini2f</code>), while complex paper transformations benefit from <code>$$$$</code>/Slow models (<code>opus</code>, <code>o1</code>).</li><li><strong>Budget</strong>: Use cost indicators ($ - $$$$) to guide selection.</li><li><strong>Speed</strong>: If quick turnaround is needed, prefer Fast/Very Fast models.</li><li><strong>Special Capabilities</strong>: Do you need explicit reasoning (<code>sonnet45T</code>, <code>sonnet37T</code>, <code>gemini2fT</code>, <code>o1</code>, <code>o3-</code>, <code>o1-</code>, <code>gptoss</code>, <code>gptoss-</code>, <code>dsr1</code>), vision (<code>gpt5</code>, <code>gpt5pro</code>, <code>gpt4o</code>, <code>gemini*</code>), native PDF/audio (<code>gemini*</code>), or very large context (<code>gemini*</code>, <code>gpt41</code>, <code>gpt5</code>, <code>gpt5pro</code>)?</li></ul><p>Experimentation is often key to finding the best model for your specific needs and writing style.</p><h2 id="model-configuration" tabindex="-1">Model Configuration <a class="header-anchor" href="#model-configuration" aria-label="Permalink to &quot;Model Configuration&quot;">​</a></h2><p>You can customize which models appear in the TeXRA dropdown list via VS Code Settings (<code>Ctrl+,</code>). Search for <code>texra.models</code> and edit the JSON array. Here are the defaults:</p><div class="tip custom-block"><p class="custom-block-title">Model Availability</p><p>The specific models available by default and their identifiers (<code>sonnet45</code>, <code>gpt5</code>, <code>gpt5pro</code>, etc.) are maintained by the TeXRA developers and may change in future updates based on new releases and performance evaluations.</p></div><div class="language-json vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">json</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;texra.models&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: [</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">  &quot;gemini25p&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">,</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">  &quot;gemini25f&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">,</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">  &quot;opus41T&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">,</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">  &quot;sonnet45T&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">,</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">  &quot;sonnet4T&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">,</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">  &quot;gpt5&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">,</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">  &quot;gpt41&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">,</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">  &quot;deepseek&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">,</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">  &quot;deepseekT&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">,</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">  &quot;kimi2&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">,</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">  &quot;qwen3max&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">,</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">  &quot;grok4&quot;</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">]</span></span></code></pre></div><h3 id="instruction-polishing-model" tabindex="-1">Instruction Polishing Model <a class="header-anchor" href="#instruction-polishing-model" aria-label="Permalink to &quot;Instruction Polishing Model&quot;">​</a></h3><p>TeXRA also uses a dedicated setting for polishing instruction text before an agent run. Set <code>&quot;texra.model.instructionPolishModel&quot;</code> to any short name from the enum (same as <code>texra.models</code>) to pick the model that handles this formatting step when Copilot is disabled. The default is <code>sonnet45</code>.</p><div class="language-json vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">json</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;texra.model.instructionPolishModel&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;sonnet45&quot;</span></span></code></pre></div><p>This setting is independent from the dropdown list—use it to lock polishing to a stable model while you experiment with other agents. The setting accepts any model from the same enum as <code>texra.models</code>, ensuring you can only select valid models.</p><h2 id="using-openrouter" tabindex="-1">Using OpenRouter <a class="header-anchor" href="#using-openrouter" aria-label="Permalink to &quot;Using OpenRouter&quot;">​</a></h2><p>To access models not directly integrated (like Llama or Qwen), find alternative pricing, or ensure access if a direct API key isn&#39;t available, you can use <a href="https://openrouter.ai/" target="_blank" rel="noreferrer">OpenRouter</a>.</p><ol><li>Get an <a href="https://openrouter.ai/" target="_blank" rel="noreferrer">OpenRouter</a> API key.</li><li>Add the key using the <code>TeXRA: Set API Key</code> command (select OpenRouter).</li><li>Enable OpenRouter in VS Code Settings: <code>&quot;texra.model.useOpenRouter&quot;: true</code>.</li></ol><p>When enabled, TeXRA will route API calls <strong>for all compatible models</strong> (including Anthropic, OpenAI, Google, DeepSeek, Grok, etc., if supported by OpenRouter) through OpenRouter instead of their direct APIs.</p><h2 id="streaming-support" tabindex="-1">Streaming Support <a class="header-anchor" href="#streaming-support" aria-label="Permalink to &quot;Streaming Support&quot;">​</a></h2><p>For long responses or reasoning-heavy models, you can enable streaming to see incremental results. This is often more robust for complex tasks.</p><p>Configure streaming in VS Code Settings:</p><div class="language-json vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">json</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;">// General streaming toggle (applies if specific model type toggle isn&#39;t set)</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;texra.model.useStreaming&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">false</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">,</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;">// Specific toggle for Anthropic reasoning models</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;texra.model.useStreamingAnthropicReasoning&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">false</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">,</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;">// Specific toggle for OpenAI reasoning models</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;texra.model.useStreamingOpenAIReasoning&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">false</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">,</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;">// Specific toggle for Google models</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;texra.model.useStreamingGoogle&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">false</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;">// Similar configuration exists for DeepSeek and OpenRouter models</span></span></code></pre></div><h2 id="next-steps" tabindex="-1">Next Steps <a class="header-anchor" href="#next-steps" aria-label="Permalink to &quot;Next Steps&quot;">​</a></h2><ul><li><a href="./built-in-agents.html">Built-in Agents</a>: See which agents work well with different models.</li><li><a href="./configuration.html">Configuration</a>: Learn about other model-related settings like streaming.</li><li><a href="./openai-responses-api.html">OpenAI Responses API</a>: Overview of the new API used when <code>useOpenAIResponsesAPI</code> is enabled.</li></ul>`,67)]))}const y=e(a,[["render",n]]);export{c as __pageData,y as default};
