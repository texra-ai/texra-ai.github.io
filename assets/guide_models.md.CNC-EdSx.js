import{_ as e,c as l,o as i,a2 as s}from"./chunks/framework.D30IVGRC.js";const c=JSON.parse('{"title":"AI Models","description":"","frontmatter":{},"headers":[],"relativePath":"guide/models.md","filePath":"guide/models.md"}'),a={name:"guide/models.md"};function n(d,t,o,r,g,h){return i(),l("div",null,t[0]||(t[0]=[s(`<h1 id="ai-models" tabindex="-1">AI Models <a class="header-anchor" href="#ai-models" aria-label="Permalink to &quot;AI Models&quot;">​</a></h1><p>TeXRA supports a variety of language models from different providers, allowing you to choose the best fit for your task&#39;s complexity, required speed, and budget (think of it as choosing your research assistant&#39;s brain!). This guide provides an overview of the models available by default.</p><h2 id="model-providers-overview" tabindex="-1">Model Providers Overview <a class="header-anchor" href="#model-providers-overview" aria-label="Permalink to &quot;Model Providers Overview&quot;">​</a></h2><p>TeXRA primarily integrates with models from:</p><ol><li><strong>Anthropic</strong> (Claude family)</li><li><strong>OpenAI</strong> (GPT and O-series families)</li><li><strong>Google</strong> (Gemini family)</li><li><strong>Other Providers</strong> (via OpenRouter, including Grok, DeepSeek)</li></ol><p>You can select the desired model from the dropdown list in the TeXRA UI.</p><h2 id="default-model-selection" tabindex="-1">Default Model Selection <a class="header-anchor" href="#default-model-selection" aria-label="Permalink to &quot;Default Model Selection&quot;">​</a></h2><p>Here&#39;s a quick comparison of the models available by default in TeXRA:</p><h3 id="anthropic-models" tabindex="-1">Anthropic Models <a class="header-anchor" href="#anthropic-models" aria-label="Permalink to &quot;Anthropic Models&quot;">​</a></h3><p>Known for strong instruction following and context handling.</p><table tabindex="0"><thead><tr><th style="text-align:left;">Model ID</th><th style="text-align:left;">Key Strength / Use Case</th><th style="text-align:left;">Relative Cost</th><th style="text-align:left;">Relative Speed</th><th style="text-align:left;">Notes</th></tr></thead><tbody><tr><td style="text-align:left;"><code>opus</code></td><td style="text-align:left;">High quality, complex tasks</td><td style="text-align:left;">$$$$</td><td style="text-align:left;">Slow</td><td style="text-align:left;"></td></tr><tr><td style="text-align:left;"><code>sonnet37</code></td><td style="text-align:left;">Strong all-rounder, good context</td><td style="text-align:left;">$$$</td><td style="text-align:left;">Medium</td><td style="text-align:left;"></td></tr><tr><td style="text-align:left;"><code>sonnet37T</code></td><td style="text-align:left;"><code>sonnet37</code> with explicit reasoning steps</td><td style="text-align:left;">$$$</td><td style="text-align:left;">Medium</td><td style="text-align:left;">Good for math, complex logic</td></tr><tr><td style="text-align:left;"><code>sonnet35</code></td><td style="text-align:left;">Good balance of quality/cost (older Sonnet)</td><td style="text-align:left;">$$$</td><td style="text-align:left;">Medium</td><td style="text-align:left;"></td></tr></tbody></table><h3 id="openai-models" tabindex="-1">OpenAI Models <a class="header-anchor" href="#openai-models" aria-label="Permalink to &quot;OpenAI Models&quot;">​</a></h3><p>Known for strong reasoning and creative capabilities.</p><table tabindex="0"><thead><tr><th style="text-align:left;">Model ID</th><th style="text-align:left;">Key Strength / Use Case</th><th style="text-align:left;">Relative Cost</th><th style="text-align:left;">Relative Speed</th><th style="text-align:left;">Notes</th></tr></thead><tbody><tr><td style="text-align:left;"><code>o1</code></td><td style="text-align:left;">Advanced reasoning, math, figures</td><td style="text-align:left;">$$$$</td><td style="text-align:left;">Slow</td><td style="text-align:left;">Explicit reasoning</td></tr><tr><td style="text-align:left;"><code>gpt45</code></td><td style="text-align:left;">High quality, vision (Preview)</td><td style="text-align:left;">$$$$</td><td style="text-align:left;">Medium</td><td style="text-align:left;"></td></tr><tr><td style="text-align:left;"><code>gpt41</code></td><td style="text-align:left;">Long-context vision, powerful</td><td style="text-align:left;">$$$</td><td style="text-align:left;">Medium</td><td style="text-align:left;">1M tokens context</td></tr><tr><td style="text-align:left;"><code>gpt41-</code></td><td style="text-align:left;">Long-context vision, cost-effective</td><td style="text-align:left;">$$</td><td style="text-align:left;">Medium</td><td style="text-align:left;">1M tokens context, mini</td></tr><tr><td style="text-align:left;"><code>gpt41--</code></td><td style="text-align:left;">Long-context vision, cheapest</td><td style="text-align:left;">$</td><td style="text-align:left;">Medium</td><td style="text-align:left;">1M tokens context, nano</td></tr><tr><td style="text-align:left;"><code>gpt4o</code></td><td style="text-align:left;">Strong all-rounder, vision</td><td style="text-align:left;">$$$</td><td style="text-align:left;">Medium</td><td style="text-align:left;">Good default choice</td></tr><tr><td style="text-align:left;"><code>gpt4ol</code></td><td style="text-align:left;">Latest <code>gpt4o</code>, potentially better</td><td style="text-align:left;">$$$</td><td style="text-align:left;">Medium</td><td style="text-align:left;"></td></tr><tr><td style="text-align:left;"><code>o3-</code></td><td style="text-align:left;">Fast reasoning</td><td style="text-align:left;">$$$</td><td style="text-align:left;">Fast</td><td style="text-align:left;"><code>o3-mini</code></td></tr><tr><td style="text-align:left;"><code>o1-</code></td><td style="text-align:left;">Fast reasoning (smaller <code>o1</code>)</td><td style="text-align:left;">$$$</td><td style="text-align:left;">Fast</td><td style="text-align:left;"><code>o1-mini</code></td></tr></tbody></table><h3 id="google-models" tabindex="-1">Google Models <a class="header-anchor" href="#google-models" aria-label="Permalink to &quot;Google Models&quot;">​</a></h3><p>Known for large context windows, multimodality, and speed/cost efficiency.</p><table tabindex="0"><thead><tr><th style="text-align:left;">Model ID</th><th style="text-align:left;">Key Strength / Use Case</th><th style="text-align:left;">Relative Cost</th><th style="text-align:left;">Relative Speed</th><th style="text-align:left;">Notes</th></tr></thead><tbody><tr><td style="text-align:left;"><code>gemini25p</code></td><td style="text-align:left;">Strong reasoning, vision, large context</td><td style="text-align:left;">$$$</td><td style="text-align:left;">Medium</td><td style="text-align:left;">Latest Pro model</td></tr><tr><td style="text-align:left;"><code>gemini2p</code></td><td style="text-align:left;">Good reasoning, vision, very large context</td><td style="text-align:left;">$$</td><td style="text-align:left;">Medium</td><td style="text-align:left;"></td></tr><tr><td style="text-align:left;"><code>gemini25f</code></td><td style="text-align:left;">Fast reasoning, large context</td><td style="text-align:left;">$$</td><td style="text-align:left;">Fast</td><td style="text-align:left;">Latest Flash model</td></tr><tr><td style="text-align:left;"><code>gemini2fT</code></td><td style="text-align:left;"><code>gemini2f</code> with explicit reasoning steps</td><td style="text-align:left;">$</td><td style="text-align:left;">Fast</td><td style="text-align:left;"></td></tr><tr><td style="text-align:left;"><code>gemini2f</code></td><td style="text-align:left;">Fastest, most cost-effective, vision</td><td style="text-align:left;">$</td><td style="text-align:left;">Very Fast</td><td style="text-align:left;">Good for simple tasks, native PDF/audio</td></tr></tbody></table><h3 id="deepseek-models" tabindex="-1">DeepSeek Models <a class="header-anchor" href="#deepseek-models" aria-label="Permalink to &quot;DeepSeek Models&quot;">​</a></h3><p>Strong technical and coding performance, cost-effective.</p><table tabindex="0"><thead><tr><th style="text-align:left;">Model ID</th><th style="text-align:left;">Key Strength / Use Case</th><th style="text-align:left;">Relative Cost</th><th style="text-align:left;">Relative Speed</th><th style="text-align:left;">Notes</th></tr></thead><tbody><tr><td style="text-align:left;"><code>DSV3</code></td><td style="text-align:left;">Good coding &amp; general tasks</td><td style="text-align:left;">$</td><td style="text-align:left;">Fast</td><td style="text-align:left;">DeepSeek V3 Chat</td></tr><tr><td style="text-align:left;"><code>DSR1</code></td><td style="text-align:left;">Advanced reasoning</td><td style="text-align:left;">$$</td><td style="text-align:left;">Medium</td><td style="text-align:left;">DeepSeek R1</td></tr></tbody></table><h3 id="moonshot-kimi-models" tabindex="-1">Moonshot Kimi Models <a class="header-anchor" href="#moonshot-kimi-models" aria-label="Permalink to &quot;Moonshot Kimi Models&quot;">​</a></h3><p>High context models from Moonshot, suitable for complex reasoning and large documents.</p><table tabindex="0"><thead><tr><th style="text-align:left;">Model ID</th><th style="text-align:left;">Key Strength / Use Case</th><th style="text-align:left;">Relative Cost</th><th style="text-align:left;">Relative Speed</th><th style="text-align:left;">Notes</th></tr></thead><tbody><tr><td style="text-align:left;"><code>kimit</code></td><td style="text-align:left;">Detailed reasoning with vision</td><td style="text-align:left;">$$$</td><td style="text-align:left;">Medium</td><td style="text-align:left;">Kimi Thinking Preview</td></tr><tr><td style="text-align:left;"><code>kimi</code></td><td style="text-align:left;">Large context, general tasks</td><td style="text-align:left;">$$</td><td style="text-align:left;">Medium</td><td style="text-align:left;">128k context</td></tr><tr><td style="text-align:left;"><code>kimiv</code></td><td style="text-align:left;">Vision-enabled variant</td><td style="text-align:left;">$$</td><td style="text-align:left;">Medium</td><td style="text-align:left;">128k context, vision</td></tr></tbody></table><h3 id="dashscope-qwen-models" tabindex="-1">DashScope Qwen Models <a class="header-anchor" href="#dashscope-qwen-models" aria-label="Permalink to &quot;DashScope Qwen Models&quot;">​</a></h3><p>Cost-effective models from Alibaba with strong multilingual capabilities.</p><table tabindex="0"><thead><tr><th style="text-align:left;">Model ID</th><th style="text-align:left;">Key Strength / Use Case</th><th style="text-align:left;">Relative Cost</th><th style="text-align:left;">Relative Speed</th><th style="text-align:left;">Notes</th></tr></thead><tbody><tr><td style="text-align:left;"><code>qwenmax</code></td><td style="text-align:left;">Advanced reasoning</td><td style="text-align:left;">$$</td><td style="text-align:left;">Medium</td><td style="text-align:left;">Qwen Max</td></tr><tr><td style="text-align:left;"><code>qwenplus</code></td><td style="text-align:left;">Large context general purpose</td><td style="text-align:left;">$$</td><td style="text-align:left;">Medium</td><td style="text-align:left;">Qwen Plus</td></tr><tr><td style="text-align:left;"><code>qwenturbo</code></td><td style="text-align:left;">Fast responses</td><td style="text-align:left;">$</td><td style="text-align:left;">Fast</td><td style="text-align:left;">Qwen Turbo</td></tr></tbody></table><h3 id="grok-xai-models" tabindex="-1">Grok / xAI Models <a class="header-anchor" href="#grok-xai-models" aria-label="Permalink to &quot;Grok / xAI Models&quot;">​</a></h3><p>Large context models from xAI.</p><table tabindex="0"><thead><tr><th style="text-align:left;">Model ID</th><th style="text-align:left;">Key Strength / Use Case</th><th style="text-align:left;">Relative Cost</th><th style="text-align:left;">Relative Speed</th><th style="text-align:left;">Notes</th></tr></thead><tbody><tr><td style="text-align:left;"><code>grok3</code></td><td style="text-align:left;">Large context, alternative reasoning</td><td style="text-align:left;">$$$</td><td style="text-align:left;">Medium</td><td style="text-align:left;">xAI Grok 3</td></tr><tr><td style="text-align:left;"><code>grok3-</code></td><td style="text-align:left;">Faster Grok 3 (mini)</td><td style="text-align:left;">$$</td><td style="text-align:left;">Fast</td><td style="text-align:left;">xAI Grok 3 Mini</td></tr></tbody></table><h3 id="other-models-available-primarily-via-openrouter" tabindex="-1">Other Models (Available Primarily via OpenRouter) <a class="header-anchor" href="#other-models-available-primarily-via-openrouter" aria-label="Permalink to &quot;Other Models (Available Primarily via OpenRouter)&quot;">​</a></h3><p>These models are generally accessed by enabling OpenRouter in settings.</p><table tabindex="0"><thead><tr><th style="text-align:left;">Model ID</th><th style="text-align:left;">Key Strength / Use Case</th><th style="text-align:left;">Provider</th><th style="text-align:left;">Relative Cost</th><th style="text-align:left;">Relative Speed</th></tr></thead><tbody><tr><td style="text-align:left;"><code>llama31</code></td><td style="text-align:left;">Strong open model, large context</td><td style="text-align:left;">Meta</td><td style="text-align:left;">$$$</td><td style="text-align:left;">Medium</td></tr><tr><td style="text-align:left;"><code>qvq-72b</code></td><td style="text-align:left;">Strong multi-lingual</td><td style="text-align:left;">Qwen/Alibaba</td><td style="text-align:left;">$$</td><td style="text-align:left;">Medium</td></tr></tbody></table><p><em>Relative Cost/Speed are estimates: $ = Low/Fast, $$$$ = High/Slow.</em></p><h2 id="choosing-the-right-model" tabindex="-1">Choosing the Right Model <a class="header-anchor" href="#choosing-the-right-model" aria-label="Permalink to &quot;Choosing the Right Model&quot;">​</a></h2><p>Consider these factors:</p><ul><li><strong>Task Complexity</strong>: Simple corrections might only need a <code>$</code>/Fast model (<code>gemini2f</code>), while complex paper transformations benefit from <code>$$$$</code>/Slow models (<code>opus</code>, <code>o1</code>).</li><li><strong>Budget</strong>: Use cost indicators ($ - $$$$) to guide selection.</li><li><strong>Speed</strong>: If quick turnaround is needed, prefer Fast/Very Fast models.</li><li><strong>Special Capabilities</strong>: Do you need explicit reasoning (<code>sonnet37T</code>, <code>gemini2fT</code>, <code>o1</code>, <code>o3-</code>, <code>o1-</code>, <code>DSR1</code>), vision (<code>gpt4o</code>, <code>gemini*</code>), native PDF/audio (<code>gemini*</code>), or very large context (<code>gemini*</code>, <code>gpt41</code>)?</li></ul><p>Experimentation is often key to finding the best model for your specific needs and writing style.</p><h2 id="model-configuration" tabindex="-1">Model Configuration <a class="header-anchor" href="#model-configuration" aria-label="Permalink to &quot;Model Configuration&quot;">​</a></h2><p>You can customize which models appear in the TeXRA dropdown list via VS Code Settings (<code>Ctrl+,</code>). Search for <code>texra.models</code> and edit the JSON array. Here are the defaults:</p><div class="tip custom-block"><p class="custom-block-title">Model Availability</p><p>The specific models available by default and their identifiers (<code>sonnet37</code>, <code>gpt4o</code>, etc.) are maintained by the TeXRA developers and may change in future updates based on new releases and performance evaluations.</p></div><div class="language-json vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">json</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;texra.models&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: [</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">  &quot;sonnet37T&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">,</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">  &quot;sonnet37&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">,</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">  &quot;o3&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">,</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">  &quot;o4-&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">,</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">  &quot;o3-&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">,</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">  &quot;o1&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">,</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">  &quot;gpt41&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">,</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">  &quot;gpt4o&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">,</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">  &quot;gemini25p&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">,</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">  &quot;gemini25f&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">,</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">  &quot;gemini2fT&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">,</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">  &quot;DSV3&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">,</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">  &quot;DSR1&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">,</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">  &quot;grok3&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">,</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">  &quot;qwenplus&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">,</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">  &quot;kimit&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">,</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">  &quot;kimiv&quot;</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">]</span></span></code></pre></div><h2 id="using-openrouter" tabindex="-1">Using OpenRouter <a class="header-anchor" href="#using-openrouter" aria-label="Permalink to &quot;Using OpenRouter&quot;">​</a></h2><p>To access models not directly integrated (like Llama or Qwen), find alternative pricing, or ensure access if a direct API key isn&#39;t available, you can use <a href="https://openrouter.ai/" target="_blank" rel="noreferrer">OpenRouter</a>.</p><ol><li>Get an <a href="https://openrouter.ai/" target="_blank" rel="noreferrer">OpenRouter</a> API key.</li><li>Add the key using the <code>TeXRA: Set API Key</code> command (select OpenRouter).</li><li>Enable OpenRouter in VS Code Settings: <code>&quot;texra.model.useOpenRouter&quot;: true</code>.</li></ol><p>When enabled, TeXRA will route API calls <strong>for all compatible models</strong> (including Anthropic, OpenAI, Google, DeepSeek, Grok, etc., if supported by OpenRouter) through OpenRouter instead of their direct APIs.</p><h2 id="streaming-support" tabindex="-1">Streaming Support <a class="header-anchor" href="#streaming-support" aria-label="Permalink to &quot;Streaming Support&quot;">​</a></h2><p>For long responses or reasoning-heavy models, you can enable streaming to see incremental results. This is often more robust for complex tasks.</p><p>Configure streaming in VS Code Settings:</p><div class="language-json vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">json</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;">// General streaming toggle (applies if specific model type toggle isn&#39;t set)</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;texra.model.useStreaming&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">false</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">,</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;">// Specific toggle for Anthropic reasoning models</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;texra.model.useStreamingAnthropicReasoning&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">false</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">,</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;">// Specific toggle for OpenAI reasoning models</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;texra.model.useStreamingOpenAIReasoning&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">false</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;">// Similar configuration for Google/DeepSeek/OpenRouter models</span></span></code></pre></div><h2 id="next-steps" tabindex="-1">Next Steps <a class="header-anchor" href="#next-steps" aria-label="Permalink to &quot;Next Steps&quot;">​</a></h2><ul><li><a href="./built-in-agents.html">Built-in Agents</a>: See which agents work well with different models.</li><li><a href="./configuration.html">Configuration</a>: Learn about other model-related settings like streaming.</li></ul>`,51)]))}const f=e(a,[["render",n]]);export{c as __pageData,f as default};
